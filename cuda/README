+--------------------------+
| PyTorch & CUDA extension |
+--------------------------+

To compile the CUDA extension for PyTorch:
$ cd pytorch_gpu/
$ sh compile.sh

Test the CUDA extension with the PyTorch autograd function:
$ python3 test_PyTorch.py

* The CUDA kernels (forward/backward) and the binding functions are implemented in pytorch_gpu/RisiContraction_18.cu.
* The PyTorch autograd function is in pytorch_gpu/RisiContraction_18_Op.py.
* For CUDA extension compilation: pytorch_gpu/setup.py and pytorch_gpu/compile.sh.

+--------------------------------+
| Mini GraphFlow (pure C++/CUDA) |
+--------------------------------+

To run all the experiments on GraphFlow (mini version):
$ sh test_GraphFlow.sh

Test for the CPU/C++ contraction of GraphFlow:
$ g++ test_GraphFlow.cpp -o test_GraphFlow_cpu; ./test_GraphFlow_cpu 10 20

Test for the GPU/CUDA contraction of GraphFlow:
$ nvcc test_GraphFlow.cu -o test_GraphFlow_cuda; ./test_GraphFlow_cuda 10 20

Mini version of the deep learning framework is in GraphFlow_gpu/ directory:
* Entity: Entity.h
* Vector: Vector.h
* Matrix: Matrix.h
* 3-dimensional tensor: Tensor3D.h
* 4-dimensional tensor: Tensor4D.h
* CPU/C++ contraction operator: RisiContraction_18.h
* GPU/CUDA contraction operator: RisiContraction_18_gpu.h